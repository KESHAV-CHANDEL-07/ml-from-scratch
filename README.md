# 🤖 Machine Learning From Scratch — by Keshav Chandel

Welcome to my personal ML journey where I build everything from **scratch using only Python and NumPy**, focusing on **deep mathematical understanding**, **matrix operations**, and **step-by-step logic** — not just using prebuilt libraries.

---

## 📚 What I've Covered So Far

### ✅ Phase 1: Simple Linear Regression
* Implemented 1D linear regression from scratch.
* Learned cost function (MSE), gradient descent, and matrix dot products.
* Manually predicted new values and visualized loss reduction.  

📁 Folder: `Phase_1_Simple_Regression/`

---

### ✅ Phase 2: Polynomial Regression
* Extended linear regression to fit **nonlinear data** using polynomial terms (x², x³, ...).
* Observed overfitting vs underfitting.
* Trained and predicted on curved data manually.  

📁 Folder: `Phase_2_Polynomial_Regression/`

---

### ✅ Phase 3: Multiple Linear Regression
* Added support for multiple input features.
* Calculated cost, gradients, and updates using **matrix math**.
* Applied on example datasets to predict values with multiple variables.  

📁 Folder: `Phase_3_Multiple_Linear_Regression/`

---

### ✅ Phase 4: Logistic Regression
* Implemented binary classification using logistic regression from scratch.
* Applied sigmoid activation, binary cross-entropy loss, and matrix-based gradients.
* Built a diabetes prediction model with user input.
* Evaluated with accuracy and confusion matrix.  

📁 Folder: `Phase_4_Logistic_Regression/`

---

### ✅ Phase 5: Regularization (Ridge & Lasso)
* Implemented Ridge (L2) and Lasso (L1) Regression from scratch.
* Applied Ridge on a used car price prediction dataset.
* Applied Lasso on a student performance prediction dataset.
* Compared predictions across different λ values (0, 0.1, 1, 10).
* Observed how Ridge reduces overfitting and Lasso drives some weights to zero.  

📁 Folder: `Phase_5_Regularization/`

---

### ✅ Phase 6: Decision Tree Classifier
* Built a decision tree classifier fully from scratch.
* Calculated **entropy and Gini index** to measure uncertainty.
* Selected best features to split using a simple greedy algorithm.
* Handled categorical variables with binary splits.
* Visualized the tree using Graphviz.  

📁 Folder: `Phase_6_Decision_Tree/`

---

### ✅ Phase 7: Random Forest Classifier
* Built a Random Forest Classifier from scratch.
* Used **bootstrap sampling + majority voting**.
* Implemented **stratified train-test split**.
* Evaluated using custom accuracy, precision, recall, and F1-score functions.
* Tested on heart disease datasets with binary and multiclass outputs.  

📁 Folder: `Phase_7_Random_Forest/`

---

### ✅ Phase 8: K-Nearest Neighbors (KNN)
* Implemented KNN from scratch.
* Applied it to **SMS spam detection**.
* Evaluated predictions and tested different `k` values.  

📁 Folder: `Phase_8_KNN/`

---

### ✅ Phase 9: Naive Bayes
* Implemented a simple Naive Bayes classifier from scratch.
* Focused on numerical features and probability calculation.
* Tested on basic datasets.  

📁 Folder: `Phase_9_Naive_Bayes/`

---

### ✅ Phase 10: Support Vector Machine (SVM)
* Implemented basic SVM from scratch.
* Focused on **linear separable data** and margin calculation.
* Learned weight updates manually.  

📁 Folder: `Phase_10_SVM/`

---

### ✅ Phase 11: Neural Network (1-Layer / CNN)
* Built a single-layer dense neural network.
* Implemented **forward propagation, ReLU, softmax**, and **cross-entropy loss**.
* Added **gradient descent weight updates**.
* Built a simple CNN using NumPy and later PyTorch for GPU acceleration.
* Tested on MNIST dataset with batch processing.  

📁 Folder: `Phase_11_NN/`

---

## 🧠 Why I'm Doing This
I want to:
* Master **core math and matrix logic** behind ML.
* Build a **strong foundation** for advanced models.
* Apply this in **real-world projects**.
* Become internship-ready with a **hands-on + deep understanding** profile.

---

## 🧰 Tools Used
* Python 3
* NumPy
* Pandas (for preprocessing)
* Matplotlib (for visualization)
* Graphviz (for decision tree visualization)
* PyTorch (for GPU acceleration in CNN)
* 💡 No scikit-learn or ML libraries for core logic — everything done manually.

---

## 🌐 About Me
**Name:** Keshav Chandel  
**Email:** [23bec053@nith.ac.in](mailto:23bec053@nith.ac.in)  
**Passion:** Combining Machine Learning with hardware (Raspberry Pi, Arduino, IoT)  
**Goal:** Internship-ready profile with projects showing real ML skills.

---

Thanks for checking out my journey 🚀  
**Built with ❤️ by Keshav Chandel**
