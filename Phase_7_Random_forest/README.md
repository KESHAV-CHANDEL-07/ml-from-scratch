# ğŸŒ³ Random Forest Classifier â€” From Scratch vs Scikit-learn

This project demonstrates a complete **Random Forest Classifier built from scratch using Python and NumPy**, alongside a comparison with the `scikit-learn` implementation, applied on two variations of the **Heart Disease Dataset**.

---

## ğŸ› ï¸ What We Built

- âœ… A **Decision Tree Classifier** using the **Gini Index**
- âœ… A **Random Forest Classifier** using:
  - Bootstrap sampling
  - Majority voting
- âœ… Manual **stratified train-test split** (no `sklearn`)
- âœ… Custom evaluation metrics:
  - **Accuracy**
  - **Precision**
  - **Recall**
  - **F1-Score**
- âœ… **Tree visualization** using `graphviz` and `scikit-learn` (for interpretability)

---

## ğŸ§  Skills Demonstrated

- Data preprocessing and cleaning
- Manual implementation of ensemble algorithms
- Mathematical understanding of:
  - Gini impurity
  - Tree splitting logic
  - Bias-variance tradeoff
- Handling class imbalance
- Visual debugging with decision tree visualization
- Building interpretable ML from scratch
- Metric-based model evaluation
- Python scripting using NumPy, Pandas, Graphviz

---

## ğŸ“ˆ Datasets Used

### 1. ğŸ§ª UCI Heart Disease Dataset (`processed.cleveland.data`)
- Multiclass targets: `0, 1, 2, 3, 4` indicating increasing severity.
- Imbalanced, with higher severity classes underrepresented.

### 2. â¤ï¸ Kaggleâ€™s Heart Dataset (`heart.csv`)
- Balanced binary classification: `0` (No Disease), `1` (Disease)
- Used for final model comparisons and visualization

---

## ğŸ“š Learning Journey

### âœ… Phase 1: From Scratch
- Built `build_tree`, `predict_tree`, `build_random_forest`, and `predict_forest` from scratch using NumPy.
- Implemented majority voting + bootstrap sampling.

### âœ… Phase 2: Faced Class Imbalance
- UCI dataset had too few samples for classes `2`, `3`, and `4`.
- Precision and recall for higher classes dropped to near **0%**.
- Accuracy stagnated around **48â€“55%**.

### âœ… Phase 3: Hyperparameter Tuning
- Increased `max_depth` and number of trees (`n_trees`).
- Slight improvements seen but imbalance persisted.

### âœ… Phase 4: Binary Classification
- Converted target into:
  - `0` â†’ No Disease
  - `1` â†’ Disease (combining classes 1â€“4)
- Performance boosted:
  - Accuracy reached **~93%**
  - High precision and recall for both classes

### âœ… Phase 5: Better Dataset
- Switched to **Kaggleâ€™s balanced heart.csv**
- Improved training consistency and multiclass handling

### âœ… Phase 6: Scikit-learn Benchmark
- Used `DecisionTreeClassifier` and `RandomForestClassifier` from `sklearn`
- Confirmed that accuracy from scratch and sklearn were comparable
- Visualized trees using `graphviz`

---

## ğŸŒ³ Tree Visualization

We used **Graphviz** and `sklearn.tree.export_graphviz()` to generate decision tree diagrams. These visualizations helped interpret model decisions with:

- Gini impurity at each node
- Feature splits and thresholds
- Class distributions
- Leaf node predictions

The tree was saved as `tree.pdf`.

---

### ğŸ§¡ Built with heart by **Keshav Chandel**
